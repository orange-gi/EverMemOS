"""å…±äº«å·¥å…·å‡½æ•°æ¨¡å— - ç”¨äºè®°å¿†æå–å’Œå¯¹è¯ç³»ç»Ÿ

æœ¬æ¨¡å—æä¾›å…¬å…±çš„å·¥å…·å‡½æ•°ï¼Œä¾› extract_memory.py å’Œ chat_with_memory.py å…±åŒä½¿ç”¨ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- MongoDB è¿æ¥å’Œåˆå§‹åŒ–
- Profile åŠ è½½å’Œç®¡ç†
- MemCell æŸ¥è¯¢
- æ£€ç´¢ç­–ç•¥ï¼ˆå‘é‡ç›¸ä¼¼åº¦ï¼‰
- æ—¶é—´åºåˆ—åŒ–å·¥å…·
- æ€§èƒ½ç›‘æ§å’Œè¿›åº¦è¿½è¸ªï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
"""

import json
import os
import time
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from dataclasses import dataclass, field

from agentic_layer.vectorize_service import get_vectorize_service


import numpy as np
import requests
from motor.motor_asyncio import AsyncIOMotorClient
from beanie import init_beanie

# å¯¼å…¥é¡¹ç›®ä¸­çš„æ–‡æ¡£æ¨¡å‹
import sys

PROJECT_ROOT = Path(__file__).resolve().parents[1]
src_path = str(PROJECT_ROOT / "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨è·¯å¾„ä¸­
project_root = str(PROJECT_ROOT)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.infra_layer.adapters.out.persistence.document.memory.memcell import (
    MemCell as DocMemCell,
)
from demo.memory_config import MongoDBConfig, EmbeddingConfig


def cosine_similarity(vec1, vec2):
    """
    è®¡ç®—ä¸¤ä¸ª numpy å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚

    å‚æ•°:
    vec1 (np.ndarray): ç¬¬ä¸€ä¸ªå‘é‡ã€‚
    vec2 (np.ndarray): ç¬¬äºŒä¸ªå‘é‡ã€‚

    è¿”å›:
    float: ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
    """
    # è®¡ç®—ç‚¹ç§¯
    dot_product = np.dot(vec1, vec2)

    # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ L2 èŒƒæ•°ï¼ˆå³å‘é‡çš„æ¨¡ï¼‰
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)

    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    # æ·»åŠ ä¸€ä¸ªå¾ˆå°çš„æ•° (epsilon) æ¥é˜²æ­¢é™¤ä»¥é›¶
    epsilon = 1e-8
    similarity = dot_product / (norm_vec1 * norm_vec2 + epsilon)

    return similarity


# ============================================================================
# æ€§èƒ½ç›‘æ§å·¥å…·ï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
# ============================================================================


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡è¿½è¸ªå™¨

    ç”¨äºè¿½è¸ªå’ŒæŠ¥å‘Šæå–è¿‡ç¨‹çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ï¼š
    - LLM è°ƒç”¨æ¬¡æ•°å’Œè€—æ—¶
    - MongoDB å†™å…¥æ¬¡æ•°å’Œè€—æ—¶
    - å‘é‡åŒ–è°ƒç”¨æ¬¡æ•°å’Œè€—æ—¶
    - MemCell å’Œ Profile æ•°é‡
    """

    total_start_time: float = 0.0
    memcell_count: int = 0
    profile_count: int = 0
    llm_calls: int = 0
    llm_total_time: float = 0.0
    mongo_writes: int = 0
    mongo_total_time: float = 0.0
    embedding_calls: int = 0
    embedding_total_time: float = 0.0

    def report(self) -> None:
        """ç”Ÿæˆå¹¶æ‰“å°æ€§èƒ½æŠ¥å‘Š"""
        total_time = time.time() - self.total_start_time

        print("\n" + "=" * 80)
        print("âš¡ æ€§èƒ½æŒ‡æ ‡æŠ¥å‘Š")
        print("=" * 80)
        print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")

        print(f"\nğŸ“Š æå–ç»“æœ:")
        print(f"  - MemCell æ•°é‡: {self.memcell_count}")
        print(f"  - Profile æ•°é‡: {self.profile_count}")

        print(f"\nğŸ¤– LLM è°ƒç”¨:")
        print(f"  - æ€»è°ƒç”¨æ¬¡æ•°: {self.llm_calls}")
        print(f"  - æ€»è€—æ—¶: {self.llm_total_time:.2f}ç§’")
        if self.llm_calls > 0:
            print(f"  - å¹³å‡è€—æ—¶: {self.llm_total_time/self.llm_calls:.2f}ç§’/æ¬¡")

        print(f"\nğŸ’¾ MongoDB å†™å…¥:")
        print(f"  - æ€»å†™å…¥æ¬¡æ•°: {self.mongo_writes}")
        print(f"  - æ€»è€—æ—¶: {self.mongo_total_time:.2f}ç§’")
        if self.mongo_writes > 0:
            print(f"  - å¹³å‡è€—æ—¶: {self.mongo_total_time/self.mongo_writes:.3f}ç§’/æ¬¡")

        print(f"\nğŸ”¢ å‘é‡åŒ–:")
        print(f"  - æ€»è°ƒç”¨æ¬¡æ•°: {self.embedding_calls}")
        print(f"  - æ€»è€—æ—¶: {self.embedding_total_time:.2f}ç§’")
        if self.embedding_calls > 0:
            print(
                f"  - å¹³å‡è€—æ—¶: {self.embedding_total_time/self.embedding_calls:.3f}ç§’/æ¬¡"
            )

        print("=" * 80 + "\n")


class ProgressTracker:
    """è¿›åº¦è¿½è¸ªå™¨

    ç”¨äºæ˜¾ç¤ºå®æ—¶è¿›åº¦æ¡ã€å¤„ç†é€Ÿåº¦å’Œé¢„è®¡å®Œæˆæ—¶é—´ã€‚
    """

    def __init__(self, total: int, desc: str = "å¤„ç†ä¸­"):
        """åˆå§‹åŒ–è¿›åº¦è¿½è¸ªå™¨

        Args:
            total: æ€»ä»»åŠ¡æ•°
            desc: ä»»åŠ¡æè¿°
        """
        self.total = total
        self.current = 0
        self.desc = desc
        self.start_time = time.time()
        self.last_update = 0

    def update(self, n: int = 1) -> None:
        """æ›´æ–°è¿›åº¦

        Args:
            n: æœ¬æ¬¡å®Œæˆçš„ä»»åŠ¡æ•°
        """
        self.current += n
        current_time = time.time()

        # æ¯ 0.5 ç§’æˆ–å®Œæˆæ—¶æ›´æ–°æ˜¾ç¤º
        if current_time - self.last_update > 0.5 or self.current >= self.total:
            elapsed = current_time - self.start_time
            rate = self.current / elapsed if elapsed > 0 else 0
            eta = (self.total - self.current) / rate if rate > 0 else 0

            pct = 100.0 * self.current / self.total if self.total > 0 else 0
            bar_len = 40
            filled = int(bar_len * self.current / self.total) if self.total > 0 else 0
            bar = 'â–ˆ' * filled + '-' * (bar_len - filled)

            print(
                f'\r{self.desc}: |{bar}| {self.current}/{self.total} [{pct:.1f}%] '
                f'{rate:.1f}it/s ETA: {eta:.0f}s',
                end='',
                flush=True,
            )

            self.last_update = current_time

            if self.current >= self.total:
                print()  # å®Œæˆåæ¢è¡Œ


# ============================================================================
# æ‰¹é‡ MongoDB å†™å…¥å™¨ï¼ˆä¼˜åŒ–ç‰ˆæ–°å¢ï¼‰
# ============================================================================


class BatchMongoWriter:
    """æ‰¹é‡ MongoDB å†™å…¥å™¨

    ä½¿ç”¨ç¼“å†²åŒºæ‰¹é‡å†™å…¥ MongoDBï¼Œå‡å°‘æ•°æ®åº“ IO æ¬¡æ•°ï¼Œæé«˜æ€§èƒ½ã€‚
    """

    def __init__(
        self, batch_size: int = 20, perf_metrics: Optional[PerformanceMetrics] = None
    ):
        """åˆå§‹åŒ–æ‰¹é‡å†™å…¥å™¨

        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            perf_metrics: æ€§èƒ½æŒ‡æ ‡è¿½è¸ªå™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.batch_size = batch_size
        self.buffer: List = []
        self.lock = asyncio.Lock()
        self.perf_metrics = perf_metrics

    async def add(self, memcell) -> None:
        """æ·»åŠ  MemCell åˆ°ç¼“å†²åŒº

        Args:
            memcell: MemCell å¯¹è±¡
        """
        async with self.lock:
            doc = self._create_doc(memcell)
            self.buffer.append(doc)

            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°æ—¶è‡ªåŠ¨åˆ·æ–°
            if len(self.buffer) >= self.batch_size:
                await self._flush()

    async def flush(self) -> None:
        """å…¬å¼€çš„åˆ·æ–°æ¥å£"""
        async with self.lock:
            await self._flush()

    async def _flush(self) -> None:
        """å†…éƒ¨åˆ·æ–°é€»è¾‘ï¼ˆæ— é”ï¼‰"""
        if not self.buffer:
            return

        start_time = time.time()
        try:
            # å¯¼å…¥æ–‡æ¡£æ¨¡å‹
            from src.infra_layer.adapters.out.persistence.document.memory.memcell import (
                MemCell as DocMemCell,
            )

            # æ‰¹é‡æ’å…¥
            await DocMemCell.insert_many(self.buffer)

            elapsed = time.time() - start_time

            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            if self.perf_metrics:
                self.perf_metrics.mongo_writes += len(self.buffer)
                self.perf_metrics.mongo_total_time += elapsed

            print(
                f"[MongoDB] æ‰¹é‡å†™å…¥ {len(self.buffer)} ä¸ªMemCellï¼Œè€—æ—¶ {elapsed:.2f}ç§’"
            )
            self.buffer.clear()

        except Exception as e:
            print(f"[MongoDB] âŒ æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
            self.buffer.clear()

    def _create_doc(self, memcell):
        """åˆ›å»º MongoDB æ–‡æ¡£å¯¹è±¡

        Args:
            memcell: MemCell å¯¹è±¡

        Returns:
            DocMemCell æ–‡æ¡£å¯¹è±¡
        """
        from src.infra_layer.adapters.out.persistence.document.memory.memcell import (
            MemCell as DocMemCell,
            DataTypeEnum,
        )
        from src.common_utils.datetime_utils import (
            from_iso_format,
            get_now_with_timezone,
        )

        # è§£ææ—¶é—´æˆ³
        ts = memcell.timestamp
        if isinstance(ts, str):
            ts_dt = from_iso_format(ts)
        elif isinstance(ts, (int, float)):
            tz = get_now_with_timezone().tzinfo
            ts_dt = datetime.fromtimestamp(float(ts), tz=tz)
        else:
            ts_dt = ts or get_now_with_timezone()

        # è·å–ä¸»ç”¨æˆ· ID
        primary_user = (
            memcell.user_id_list[0]
            if getattr(memcell, 'user_id_list', None)
            else "default"
        )

        return DocMemCell(
            user_id=primary_user,
            timestamp=ts_dt,
            summary=memcell.summary or "",
            group_id=getattr(memcell, 'group_id', None),
            participants=getattr(memcell, 'participants', None),
            type=DataTypeEnum.CONVERSATION,
            subject=getattr(memcell, 'subject', None),
            keywords=getattr(memcell, 'keywords', None),
            linked_entities=getattr(memcell, 'linked_entities', None),
            episode=getattr(memcell, 'episode', None),
            semantic_memories=getattr(memcell, 'semantic_memories', None),
            extend=getattr(memcell, 'extend', None),
        )


# ============================================================================
# MongoDB ç›¸å…³å·¥å…·
# ============================================================================


async def ensure_mongo_beanie_ready(mongo_config: MongoDBConfig) -> None:
    """åˆå§‹åŒ– MongoDB å’Œ Beanie è¿æ¥

    Args:
        mongo_config: MongoDB é…ç½®å¯¹è±¡

    Raises:
        Exception: å¦‚æœè¿æ¥å¤±è´¥
    """
    # è®¾ç½®ç¯å¢ƒå˜é‡ä¾› Beanie ä½¿ç”¨
    os.environ["MONGODB_URI"] = mongo_config.uri

    # åˆ›å»º MongoDB å®¢æˆ·ç«¯å¹¶æµ‹è¯•è¿æ¥
    client = AsyncIOMotorClient(mongo_config.uri)
    try:
        await client.admin.command('ping')
        print(f"[MongoDB] âœ… è¿æ¥æˆåŠŸ: {mongo_config.database}")
    except Exception as e:
        print(f"[MongoDB] âŒ è¿æ¥å¤±è´¥: {e}")
        raise

    # åˆå§‹åŒ– Beanie æ–‡æ¡£æ¨¡å‹
    await init_beanie(
        database=client[mongo_config.database], document_models=[DocMemCell]
    )


async def query_all_groups_from_mongodb() -> List[Dict[str, Any]]:
    """æŸ¥è¯¢æ‰€æœ‰ç¾¤ç»„ ID åŠå…¶è®°å¿†æ•°é‡

    ä½¿ç”¨èšåˆç®¡é“ç»Ÿè®¡æ¯ä¸ªç¾¤ç»„çš„ MemCell æ•°é‡ã€‚

    Returns:
        ç¾¤ç»„åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"group_id": "xxx", "memcell_count": 76}, ...]
    """
    # ä½¿ç”¨èšåˆç®¡é“ç»Ÿè®¡æ¯ä¸ªç¾¤ç»„çš„è®°å¿†æ•°é‡
    pipeline = [
        {"$match": {"group_id": {"$ne": None}}},  # è¿‡æ»¤æ‰æ²¡æœ‰ group_id çš„è®°å½•
        {"$group": {"_id": "$group_id", "count": {"$sum": 1}}},
        {"$sort": {"_id": 1}},  # æŒ‰ group_id æ’åº
    ]

    # è·å– PyMongo/Motor é›†åˆè¿›è¡ŒèšåˆæŸ¥è¯¢
    # get_pymongo_collection() åœ¨ Beanie ä¸­è¿”å› Motor é›†åˆï¼ˆå¼‚æ­¥ï¼‰
    collection = DocMemCell.get_pymongo_collection()
    cursor = collection.aggregate(pipeline)
    results = await cursor.to_list(length=None)

    groups = []
    for result in results:
        groups.append({"group_id": result["_id"], "memcell_count": result["count"]})

    return groups


async def query_memcells_by_group_and_time(
    group_id: str, start_date: datetime, end_date: datetime
) -> List[DocMemCell]:
    """æŒ‰ç¾¤ç»„å’Œæ—¶é—´èŒƒå›´æŸ¥è¯¢ MemCell

    Args:
        group_id: ç¾¤ç»„ ID
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ

    Returns:
        MemCell æ–‡æ¡£å¯¹è±¡åˆ—è¡¨
    """
    memcells = (
        await DocMemCell.find(
            {"group_id": group_id, "timestamp": {"$gte": start_date, "$lt": end_date}}
        )
        .sort("timestamp")
        .to_list()
    )

    return memcells


# ============================================================================
# Profile ç›¸å…³å·¥å…·
# ============================================================================


def load_user_profiles_from_dir(output_dir: Path) -> Dict[str, Dict[str, Any]]:
    """åŠ è½½ç›®å½•ä¸­æ‰€æœ‰ç”¨æˆ·çš„ä¸ªäºº Profile

    ä»æŒ‡å®šç›®å½•åŠ è½½æ‰€æœ‰ profile_user_*.json æ–‡ä»¶ã€‚

    Args:
        output_dir: Profile æ–‡ä»¶æ‰€åœ¨ç›®å½•

    Returns:
        ç”¨æˆ· Profile å­—å…¸ï¼Œæ ¼å¼ï¼š{"user_101": {...}, "user_102": {...}, ...}
    """
    profiles = {}

    # æŸ¥æ‰¾æ‰€æœ‰ profile_user_*.json æ–‡ä»¶
    for profile_file in output_dir.glob("profile_user_*.json"):
        try:
            with profile_file.open("r", encoding="utf-8") as fp:
                profile_data = json.load(fp)

                # ä»æ–‡ä»¶åæå– user_id
                # ä¾‹å¦‚ï¼šprofile_user_101.json -> user_101
                user_id = profile_file.stem.replace("profile_", "")
                profiles[user_id] = profile_data

        except Exception as e:
            print(f"[Profile] âš ï¸ åŠ è½½å¤±è´¥ {profile_file.name}: {e}")
            continue

    return profiles


def get_user_name_from_profile(profile: Dict[str, Any]) -> Optional[str]:
    """ä» Profile ä¸­æå–ç”¨æˆ·åç§°

    Args:
        profile: Profile æ•°æ®å­—å…¸

    Returns:
        ç”¨æˆ·åç§°ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
    """
    # å°è¯•ä»ä¸åŒå­—æ®µæå–ç”¨æˆ·å
    return profile.get("user_name") or profile.get("name") or profile.get("subject")


def get_group_name_from_profiles(profiles: Dict[str, Dict]) -> Optional[str]:
    """ä» Profile ä¸­æå–ç¾¤ç»„åç§°ï¼ˆå¦‚æœæœ‰ï¼‰

    Args:
        profiles: ç”¨æˆ· Profile å­—å…¸

    Returns:
        ç¾¤ç»„åç§°ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
    """
    for profile in profiles.values():
        group_name = profile.get("group_name")
        if group_name:
            return group_name
    return None


# ============================================================================
# æ£€ç´¢ç­–ç•¥
# ============================================================================


class RetrievalStrategy:
    """æ£€ç´¢ç­–ç•¥åŸºç±»

    ç”¨äºæ‰©å±•ä¸åŒçš„æ£€ç´¢æ–¹æ³•ï¼ˆå‘é‡ç›¸ä¼¼åº¦ã€BM25ã€æ··åˆæ£€ç´¢ç­‰ï¼‰ã€‚
    """

    def __init__(self, embedding_config: EmbeddingConfig):
        """åˆå§‹åŒ–æ£€ç´¢ç­–ç•¥

        Args:
            embedding_config: åµŒå…¥æ¨¡å‹é…ç½®
        """
        self.embedding_config = embedding_config
        self.vectorize_service = get_vectorize_service()

    async def retrieve(
        self, query: str, candidates: List[DocMemCell], top_k: int
    ) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ£€ç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            candidates: å€™é€‰ MemCell åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            æ’åºåçš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° retrieve æ–¹æ³•")


class VectorSimilarityStrategy(RetrievalStrategy):
    """åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ£€ç´¢ç­–ç•¥

    ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦å¯¹å€™é€‰ MemCell è¿›è¡Œæ’åºã€‚
    """

    async def retrieve(
        self, query: str, candidates: List[DocMemCell], top_k: int
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œæ£€ç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            candidates: å€™é€‰ MemCell åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡

        Returns:
            æ’åºåçš„æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not candidates:
            return []

        # è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
        # q_vec = self._embed_texts_http([query])
        q_vec = await self.vectorize_service.get_embedding(query)
        # if not q_vec:
        #     print("[VectorSimilarity] âŒ æŸ¥è¯¢å‘é‡åµŒå…¥å¤±è´¥")
        #     return []

        # è·å–æ–‡æ¡£çš„åµŒå…¥å‘é‡
        # q = np.array(q_vec[0], dtype=np.float32)
        # doc_vecs = self._embed_texts_http(texts)
        doc_episode_vecs = []
        for candidate in candidates:
            try:
                doc_episode_vecs.append(candidate.extend["embedding"])
            except:
                doc_episode_vecs.append([0 for _ in range(1024)])

        if len(doc_episode_vecs) != len(candidates):
            print(
                f"[VectorSimilarity] âš ï¸ åµŒå…¥å‘é‡æ•°é‡ä¸åŒ¹é…: {len(doc_episode_vecs)} != {len(candidates)}"
            )
            return []

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        scores: List[float] = []
        for v in doc_episode_vecs:
            dv = np.array(v)

            score = cosine_similarity(q_vec, dv)
            # print(score)
            scores.append(score)

        # æ’åºå¹¶è¿”å› Top-K
        ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)[
            :top_k
        ]

        # æ„å»ºç»“æœåˆ—è¡¨
        results: List[Dict[str, Any]] = []
        for m, s in ranked:
            item = {
                "event_id": str(getattr(m, "event_id", getattr(m, "id", ""))),
                "timestamp": (
                    getattr(m, "timestamp", None).isoformat()
                    if getattr(m, "timestamp", None)
                    else None
                ),
                "group_id": getattr(m, "group_id", None),
                "subject": getattr(m, "subject", None),
                "summary": getattr(m, "summary", None),
                "episode": getattr(m, "episode", None),
                "participants": getattr(m, "participants", []),
                "score": round(s, 4),
            }
            results.append(item)

        return results

    async def retrieve_semantic(
        self, query: str, candidates: List[DocMemCell], date_query: datetime, top_k: int
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦è¿›è¡Œæ£€ç´¢

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²
            candidates: å€™é€‰ MemCell åˆ—è¡¨
            date_query: æ—¥æœŸæŸ¥è¯¢æ¡ä»¶
            top_k: è¿”å›ç»“æœæ•°é‡
        """
        # è·å–æŸ¥è¯¢çš„åµŒå…¥å‘é‡
        # q_vec = self._embed_texts_http([query])
        q_vec = await self.vectorize_service.get_embedding(query)

        # è§„èŒƒåŒ– date_queryï¼Œç§»é™¤æ—¶åŒºä¿¡æ¯ä»¥ä¾¿æ¯”è¾ƒ
        # è¿™æ ·å¯ä»¥å…¼å®¹ offset-naive å’Œ offset-aware çš„ datetime
        date_query_naive = (
            date_query.replace(tzinfo=None) if date_query.tzinfo else date_query
        )

        doc_semantic_memories_vecs = []
        candidate_filtered = []
        for candidate in candidates:
            # ğŸ”¥ æ£€æŸ¥ semantic_memories æ˜¯å¦ä¸º None
            if not candidate.semantic_memories:
                continue
            
            semantic_memories_vecs = []
            for semantic_memory in candidate.semantic_memories:
                # è·å– end_time å¹¶è§„èŒƒåŒ–ä¸º naive datetime ä»¥ä¾¿æ¯”è¾ƒ
                end_time = semantic_memory['end_time']

                # å…¼å®¹å¤šç§æ•°æ®æ ¼å¼ï¼šå­—ç¬¦ä¸²æˆ–datetimeå¯¹è±¡
                if isinstance(end_time, str):
                    # å­—ç¬¦ä¸²æ ¼å¼ï¼Œè§£æä¸º datetime
                    try:
                        end_time_dt = datetime.strptime(end_time, "%Y-%m-%d")
                    except ValueError:
                        # å°è¯•è§£æå¸¦æ—¶åŒºçš„ISOæ ¼å¼
                        try:
                            end_time_dt = datetime.fromisoformat(end_time)
                            # ç§»é™¤æ—¶åŒºä¿¡æ¯
                            end_time_dt = end_time_dt.replace(tzinfo=None)
                        except ValueError:
                            # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡æ­¤è®°å¿†
                            continue
                elif isinstance(end_time, datetime):
                    # å·²ç»æ˜¯ datetime å¯¹è±¡ï¼Œç§»é™¤æ—¶åŒºä¿¡æ¯
                    end_time_dt = (
                        end_time.replace(tzinfo=None) if end_time.tzinfo else end_time
                    )
                else:
                    # ä¸æ”¯æŒçš„ç±»å‹ï¼Œè·³è¿‡
                    continue

                # æ¯”è¾ƒæ—¥æœŸï¼ˆéƒ½æ˜¯ naive datetimeï¼‰
                if end_time_dt < date_query_naive:
                    continue
                semantic_memories_vecs.append(semantic_memory["embedding"])
            if len(semantic_memories_vecs) == 0:
                continue
            else:
                doc_semantic_memories_vecs.append(semantic_memories_vecs)
                candidate_filtered.append(candidate)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        scores: List[float] = []
        for v in doc_semantic_memories_vecs:
            max_score = 0
            for semantic_memory_vec in v:
                score = cosine_similarity(q_vec, np.array(semantic_memory_vec))
                if score > max_score:
                    max_score = score
            scores.append(float(max_score))
        # æ’åºå¹¶è¿”å› Top-K
        ranked = sorted(
            zip(candidate_filtered, scores), key=lambda x: x[1], reverse=True
        )[:top_k]

        # æ„å»ºç»“æœåˆ—è¡¨
        results: List[Dict[str, Any]] = []
        for m, s in ranked:
            item = {
                "event_id": str(getattr(m, "event_id", getattr(m, "id", ""))),
                "timestamp": (
                    getattr(m, "timestamp", None).isoformat()
                    if getattr(m, "timestamp", None)
                    else None
                ),
                "group_id": getattr(m, "group_id", None),
                "subject": getattr(m, "subject", None),
                "summary": getattr(m, "summary", None),
                "episode": getattr(m, "episode", None),
                "participants": getattr(m, "participants", []),
                "score": round(s, 4),
            }
            results.append(item)

        return results

    def _embed_texts_http(self, texts: List[str]) -> List[np.ndarray]:
        """é€šè¿‡ HTTP è°ƒç”¨åµŒå…¥æœåŠ¡

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []

        try:
            resp = requests.post(
                self.embedding_config.base_url,
                json={"input": texts, "model": self.embedding_config.model},
                timeout=30,  # 30ç§’è¶…æ—¶
            ).json()

            vecs = [
                np.array(item.get("embedding", []), dtype=np.float32)
                for item in resp.get("data", [])
            ]
            return vecs

        except requests.exceptions.Timeout:
            print(f"[Embedding] âŒ è¯·æ±‚è¶…æ—¶")
            return []
        except requests.exceptions.RequestException as e:
            print(f"[Embedding] âŒ è¯·æ±‚å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"[Embedding] âŒ æœªçŸ¥é”™è¯¯: {e}")
            return []


# ============================================================================
# æ—¶é—´åºåˆ—åŒ–å·¥å…·
# ============================================================================


def serialize_datetime(obj: Any) -> Any:
    """é€’å½’åºåˆ—åŒ– datetime å¯¹è±¡ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²

    Args:
        obj: è¦åºåˆ—åŒ–çš„å¯¹è±¡ï¼ˆå¯ä»¥æ˜¯ä»»æ„ç±»å‹ï¼‰

    Returns:
        åºåˆ—åŒ–åçš„å¯¹è±¡
    """
    # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›ï¼ˆé¿å…å¤„ç†å·²åºåˆ—åŒ–çš„æ—¶é—´æˆ³ï¼‰
    if isinstance(obj, str):
        return obj
    # datetime å¯¹è±¡è½¬ä¸º ISO å­—ç¬¦ä¸²
    elif isinstance(obj, datetime):
        return obj.isoformat()
    # é€’å½’å¤„ç†å­—å…¸
    elif isinstance(obj, dict):
        return {k: serialize_datetime(v) for k, v in obj.items()}
    # é€’å½’å¤„ç†åˆ—è¡¨
    elif isinstance(obj, list):
        return [serialize_datetime(item) for item in obj]
    # å¤„ç†å¯¹è±¡ï¼ˆè½¬æ¢ __dict__ï¼‰
    elif hasattr(obj, '__dict__'):
        return serialize_datetime(obj.__dict__)
    # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
    else:
        return obj
