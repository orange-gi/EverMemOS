"""
Redis time window cache manager test script

Usage:
    python src/bootstrap.py tests/test_redis_windows_cache.py

Test coverage:
1. Basic operations: append, get, clear, etc.
2. Get data by timestamp range functionality
3. Automatic cleanup mechanism (extracted cleanup function)
4. Stress testing (large amount of data processing)
5. Backward compatibility layer testing
"""

import asyncio
import time
from datetime import datetime, timedelta
from core.di.utils import get_bean
from core.observation.logger import get_logger

logger = get_logger(__name__)


class NonSerializableTestClass:
    """Test class that cannot be JSON serialized, used for Pickle fallback testing"""

    def __init__(self, name, value, multiplier=2):
        self.name = name
        self.value = value
        self.multiplier = multiplier
        self.created_at = time.time()
        # Add some complex attributes to make JSON serialization fail
        self.complex_data = {
            "set_data": {1, 2, 3, 4, 5},  # set cannot be JSON serialized
            "tuple_data": (1, 2, 3),  # tuple will become list, but we can detect it
            "bytes_data": b"hello world",  # bytes cannot be JSON serialized
        }

    def get_doubled_value(self):
        return self.value * self.multiplier

    def process_data(self, input_value):
        """Method to process data"""
        return f"{self.name}_processed_{input_value}_{self.multiplier}"

    def __eq__(self, other):
        return (
            isinstance(other, NonSerializableTestClass)
            and self.name == other.name
            and self.value == other.value
            and self.multiplier == other.multiplier
        )

    def __repr__(self):
        return f"NonSerializableTestClass(name='{self.name}', value={self.value}, multiplier={self.multiplier})"


async def test_basic_operations():
    """Test basic operations: append, get, clear, etc."""
    logger.info("Starting basic operations test...")

    # Get cache manager factory from DI container
    factory = get_bean("redis_windows_cache_factory")
    cache = await factory.create_cache_manager(
        expire_minutes=1
    )  # Use 1 minute expiration for testing

    test_key = "test_windows_cache"

    # 1. Clear test queue
    logger.info("Clearing test queue...")
    await cache.clear_queue(test_key)

    # 2. Test appending data
    logger.info("Testing data append...")
    test_data = [
        "test_string",
        {"name": "test_dict", "value": 123},
        ["test_list", 1, 2, 3],
        42,
    ]

    for data in test_data:
        success = await cache.append(test_key, data)
        assert success, f"Failed to append data: {data}"

    # 3. Verify queue size
    size = await cache.get_queue_size(test_key)
    assert size == len(test_data), f"Queue size mismatch: expected {len(test_data)}, actual {size}"
    logger.info("Queue size verification passed: %d", size)

    # 4. Use get by timestamp range (replaces get_recent)
    current_time = int(time.time() * 1000)
    one_minute_ago = current_time - 60 * 1000
    recent_data = await cache.get_by_timestamp_range(
        test_key, start_timestamp=one_minute_ago
    )
    assert len(recent_data) == len(test_data), "Retrieved data count mismatch"
    logger.info("Successfully retrieved recent data: %d items", len(recent_data))

    # 5. Check data format
    for item in recent_data:
        assert isinstance(item, dict), "Data item format error"
        assert all(
            k in item for k in ["id", "data", "timestamp", "datetime"]
        ), "Data item missing required fields"
    logger.info("Data format verification passed")

    # 6. Get queue statistics
    stats = await cache.get_queue_stats(test_key)
    assert isinstance(stats, dict), "Statistics format error"
    assert stats["total_count"] == len(test_data), "Statistics count mismatch"
    logger.info("Queue statistics: %s", stats)

    # 7. Test expiration cleanup
    logger.info("Waiting for data to expire...")
    await asyncio.sleep(70)  # Wait over 1 minute
    expired_data = await cache.get_by_timestamp_range(test_key)
    assert len(expired_data) == 0, "Data did not expire correctly"
    logger.info("Expiration cleanup verification passed")

    logger.info("✅ Basic operations test passed")


async def test_timestamp_range_query():
    """Test get data by timestamp range functionality"""
    logger.info("Starting test for getting data by timestamp range...")

    factory = get_bean("redis_windows_cache_factory")
    cache = await factory.create_cache_manager(
        expire_minutes=10, cleanup_probability=0.0
    )  # Disable random cleanup

    test_key = "test_windows_timestamp_range"

    # 1. Clear test queue
    await cache.clear_queue(test_key)

    # 2. Add test data with explicit timestamps
    logger.info("Adding test data with explicit timestamps...")
    base_timestamp = int(time.time() * 1000)
    test_data = []

    for i in range(8):
        timestamp = base_timestamp + i * 15000  # Each data item spaced 15 seconds apart
        data = {"index": i, "content": f"windows_data_{i}", "created_at": timestamp}
        test_data.append({"data": data, "timestamp": timestamp})

        success = await cache.append(test_key, data)  # Let system assign timestamp automatically
        assert success, f"Failed to add data {i+1}"

        # Slight delay to ensure timestamp differences
        await asyncio.sleep(0.1)

    logger.info("Added %d data items", len(test_data))

    # 3. Test getting all data (no time range limit)
    all_data = await cache.get_by_timestamp_range(test_key)
    assert len(all_data) == 8, f"Failed to get all data, expected 8, actual {len(all_data)}"
    logger.info("Successfully retrieved all data: %d items", len(all_data))

    # 4. Test filtering by start time
    # Get data from last 5 minutes
    five_minutes_ago = int(time.time() * 1000) - 5 * 60 * 1000
    recent_data = await cache.get_by_timestamp_range(
        test_key, start_timestamp=five_minutes_ago
    )
    assert (
        len(recent_data) == 8
    ), f"Failed to get last 5 minutes data, expected 8, actual {len(recent_data)}"
    logger.info("Start time filtering test passed")

    # 5. Test filtering by end time
    # Get data from 1 minute ago
    one_minute_ago = int(time.time() * 1000) - 60 * 1000
    old_data = await cache.get_by_timestamp_range(
        test_key, end_timestamp=one_minute_ago
    )
    logger.info("Retrieved %d items by end time filtering", len(old_data))

    # 6. Test time range filtering
    # Get data between 3 minutes ago and 1 minute ago
    three_minutes_ago = int(time.time() * 1000) - 3 * 60 * 1000
    range_data = await cache.get_by_timestamp_range(
        test_key, start_timestamp=three_minutes_ago, end_timestamp=one_minute_ago
    )
    logger.info("Retrieved %d items by time range filtering", len(range_data))

    # 7. Test limiting number of results
    limited_data = await cache.get_by_timestamp_range(test_key, limit=3)
    assert len(limited_data) == 3, f"Limiting failed, expected 3, actual {len(limited_data)}"
    logger.info("Limit count test passed")

    # 8. Test using datetime objects as timestamps
    dt_start = datetime.now() - timedelta(minutes=10)
    dt_end = datetime.now()
    dt_data = await cache.get_by_timestamp_range(
        test_key, start_timestamp=dt_start, end_timestamp=dt_end
    )
    assert len(dt_data) >= 0, "Filtering with datetime objects failed"
    logger.info("Datetime object filtering test passed, retrieved %d items", len(dt_data))

    # 9. Verify data format
    if len(all_data) > 0:
        sample_item = all_data[0]
        assert isinstance(sample_item["data"], dict), "Data format error"
        assert "timestamp" in sample_item, "Missing timestamp field"
        assert "datetime" in sample_item, "Missing formatted time field"
        logger.info("Data format verification passed")

    # 10. Clean up test data
    success = await cache.clear_queue(test_key)
    assert success, "Failed to clean up test data"

    logger.info("✅ Get data by timestamp range test passed")


async def test_json_pickle_mixed_data():
    """Test JSON and Pickle mixed data processing"""
    logger.info("Starting test for JSON and Pickle mixed data processing...")

    factory = get_bean("redis_windows_cache_factory")
    cache = await factory.create_cache_manager(
        expire_minutes=10, cleanup_probability=0.0
    )

    test_key = "test_windows_mixed_serialization"

    # 1. Clear test queue
    await cache.clear_queue(test_key)

    # 2. Prepare mixed test data
    test_data = []

    # JSON serializable data
    json_data = [
        {"type": "json", "name": "windows_dict", "timestamp": time.time()},
        ["windows_list", "json", "data", 789],
        "windows_json_string",
        {"nested": {"windows": {"data": True}}},
    ]

    # Pickle serialized data (cannot be JSON serialized)
    pickle_data = [
        NonSerializableTestClass("windows1", 150),
        NonSerializableTestClass("windows2", 250, 4),
        {
            "windows_set": {10, 20, 30},
            "windows_bytes": b"windows_binary",
            "type": "windows_complex",
        },
    ]

    # 3. Alternately add JSON and Pickle data
    logger.info("Alternately adding JSON and Pickle data...")
    all_test_data = []

    # Alternate data arrangement
    max_len = max(len(json_data), len(pickle_data))
    for i in range(max_len):
        if i < len(json_data):
            all_test_data.append(("json", json_data[i]))
        if i < len(pickle_data):
            all_test_data.append(("pickle", pickle_data[i]))

    # Add all data
    for data_type, data in all_test_data:
        success = await cache.append(test_key, data)
        assert success, f"Failed to add {data_type} data: {data}"
        test_data.append({"type": data_type, "data": data})
        logger.debug("Successfully added %s data: %s", data_type, str(data)[:50])

        # Slight delay to ensure different timestamps
        await asyncio.sleep(0.01)

    # 4. Verify total data count
    total_count = len(all_test_data)
    size = await cache.get_queue_size(test_key)
    assert size == total_count, f"Data count mismatch: expected {total_count}, actual {size}"
    logger.info(
        "Data addition completed, total: %d items (JSON: %d, Pickle: %d)",
        total_count,
        len(json_data),
        len(pickle_data),
    )

    # 5. Retrieve and verify all data
    all_data = await cache.get_by_timestamp_range(test_key)
    assert (
        len(all_data) == total_count
    ), f"Retrieved data count mismatch: expected {total_count}, actual {len(all_data)}"

    # 6. Verify correctness of each data type
    json_count = 0
    pickle_count = 0

    logger.info("Starting to verify %d items for type and content", len(all_data))
    for i, item in enumerate(all_data):
        retrieved_data = item["data"]
        logger.debug(
            "Processing item %d: %s (%s)",
            i + 1,
            type(retrieved_data),
            str(retrieved_data)[:100],
        )

        # First check special Pickle data (dictionary containing set and bytes)
        if isinstance(retrieved_data, dict) and "windows_set" in retrieved_data:
            pickle_count += 1
            # Verify dictionary with set and bytes
            assert isinstance(retrieved_data["windows_set"], set), "Set data type error"
            assert isinstance(
                retrieved_data["windows_bytes"], bytes
            ), "Bytes data type error"
            logger.debug(
                "Successfully verified dictionary with complex data: %s", retrieved_data.get("type", "unknown")
            )

        # Check Pickle data
        elif isinstance(retrieved_data, NonSerializableTestClass):
            pickle_count += 1
            # Verify Pickle object functionality
            doubled = retrieved_data.get_doubled_value()
            expected = retrieved_data.value * retrieved_data.multiplier
            assert doubled == expected, f"Pickle object function error: {doubled} != {expected}"

            # Verify complex data
            assert "set_data" in retrieved_data.complex_data, "Pickle object missing set data"
            assert (
                "bytes_data" in retrieved_data.complex_data
            ), "Pickle object missing bytes data"

            logger.debug(
                "Successfully verified Pickle object: %s, function test: %d * %d = %d",
                retrieved_data,
                retrieved_data.value,
                retrieved_data.multiplier,
                doubled,
            )

        # Check JSON data
        elif isinstance(
            retrieved_data, (dict, list, str, int, float, bool)
        ) and not isinstance(retrieved_data, NonSerializableTestClass):
            # Verify if it's one of our added JSON data
            found = False
            for original in json_data:
                if str(retrieved_data) == str(original):
                    found = True
                    break

            if found or isinstance(retrieved_data, (str, int, float, bool)):
                json_count += 1
                logger.debug("Successfully verified JSON data: %s", str(retrieved_data)[:50])

        else:
            logger.warning(
                "Unrecognized data type: %s - %s",
                type(retrieved_data),
                str(retrieved_data)[:100],
            )
            # Specifically check if it's a dictionary with complex data
            if isinstance(retrieved_data, dict):
                logger.warning("Dictionary keys: %s", retrieved_data.keys())
                for key, value in retrieved_data.items():
                    logger.warning("  %s: %s (%s)", key, value, type(value))

    # 7. Verify data type distribution
    assert json_count == len(
        json_data
    ), f"JSON data count mismatch: expected {len(json_data)}, actual {json_count}"
    assert pickle_count == len(
        pickle_data
    ), f"Pickle data count mismatch: expected {len(pickle_data)}, actual {pickle_count}"

    logger.info(
        "Data type verification completed: JSON data %d items, Pickle data %d items", json_count, pickle_count
    )

    # 8. Test timestamp range query support for mixed data
    # Get data from last 5 minutes
    five_minutes_ago = int(time.time() * 1000) - 5 * 60 * 1000
    recent_data = await cache.get_by_timestamp_range(
        test_key, start_timestamp=five_minutes_ago
    )
    assert len(recent_data) == total_count, "Timestamp range query support for mixed data failed"

    # Get first half of data
    if len(all_data) > 2:
        mid_timestamp = all_data[len(all_data) // 2]["timestamp"]
        first_half = await cache.get_by_timestamp_range(
            test_key, end_timestamp=mid_timestamp
        )
        assert len(first_half) > 0, "Timestamp range query first half data failed"

        second_half = await cache.get_by_timestamp_range(
            test_key, start_timestamp=mid_timestamp
        )
        assert len(second_half) > 0, "Timestamp range query second half data failed"

        logger.info(
            "Timestamp range query mixed data test passed: first half %d items, second half %d items",
            len(first_half),
            len(second_half),
        )

    # 9. Clean up test data
    success = await cache.clear_queue(test_key)
    assert success, "Failed to clean up test data"

    logger.info("✅ JSON and Pickle mixed data processing test passed")


async def test_pickle_performance():
    """Test Pickle serialization performance"""
    logger.info("Starting Pickle serialization performance test...")

    factory = get_bean("redis_windows_cache_factory")
    cache = await factory.create_cache_manager(expire_minutes=5)

    test_key = "test_windows_pickle_performance"

    # 1. Clear test queue
    await cache.clear_queue(test_key)

    # 2. Prepare performance test data
    batch_size = 50
    json_data = [
        {"index": i, "type": "json", "data": f"json_item_{i}"}
        for i in range(batch_size)
    ]
    pickle_data = [
        NonSerializableTestClass(f"perf_{i}", i * 10) for i in range(batch_size)
    ]

    # 3. Test JSON data performance
    logger.info("Testing JSON data write performance...")
    json_start = time.time()
    for data in json_data:
        success = await cache.append(test_key, data)
        assert success, f"JSON data write failed: {data}"
    json_write_time = time.time() - json_start

    # 4. Test Pickle data performance
    logger.info("Testing Pickle data write performance...")
    pickle_start = time.time()
    for data in pickle_data:
        success = await cache.append(test_key, data)
        assert success, f"Pickle data write failed: {data}"
    pickle_write_time = time.time() - pickle_start

    # 5. Test read performance
    logger.info("Testing mixed data read performance...")
    read_start = time.time()
    all_data = await cache.get_by_timestamp_range(test_key)
    read_time = time.time() - read_start

    # 6. Verify data correctness
    assert (
        len(all_data) == batch_size * 2
    ), f"Data count error: expected {batch_size * 2}, actual {len(all_data)}"

    # Count data types
    json_retrieved = sum(
        1
        for item in all_data
        if isinstance(item["data"], dict)
        and "type" in item["data"]
        and item["data"]["type"] == "json"
    )
    pickle_retrieved = sum(
        1 for item in all_data if isinstance(item["data"], NonSerializableTestClass)
    )

    assert (
        json_retrieved == batch_size
    ), f"JSON data read count error: expected {batch_size}, actual {json_retrieved}"
    assert (
        pickle_retrieved == batch_size
    ), f"Pickle data read count error: expected {batch_size}, actual {pickle_retrieved}"

    # 7. Output performance results
    logger.info("Performance test results:")
    logger.info(
        "  JSON write %d items: %.3f seconds (average %.3f ms/item)",
        batch_size,
        json_write_time,
        json_write_time * 1000 / batch_size,
    )
    logger.info(
        "  Pickle write %d items: %.3f seconds (average %.3f ms/item)",
        batch_size,
        pickle_write_time,
        pickle_write_time * 1000 / batch_size,
    )
    logger.info(
        "  Mixed read %d items: %.3f seconds (average %.3f ms/item)",
        len(all_data),
        read_time,
        read_time * 1000 / len(all_data),
    )

    # 8. Performance reasonableness check
    assert json_write_time < 10.0, "JSON write performance too slow"
    assert pickle_write_time < 15.0, "Pickle write performance too slow"
    assert read_time < 5.0, "Read performance too slow"

    # 9. Clean up test data
    success = await cache.clear_queue(test_key)
    assert success, "Failed to clean up test data"

    logger.info("✅ Pickle serialization performance test passed")


async def test_stress_operations():
    """Test stress operations: large amount of data processing"""
    logger.info("Starting stress test...")

    factory = get_bean("redis_windows_cache_factory")
    cache = await factory.create_cache_manager(expire_minutes=5)

    test_key = "test_windows_cache_stress"

    # 1. Clear test queue
    await cache.clear_queue(test_key)

    # 2. Batch append data
    batch_size = 1000
    logger.info("Appending %d items...", batch_size)

    start_time = time.time()
    for i in range(batch_size):
        data = {"index": i, "timestamp": time.time(), "data": f"test_data_{i}"}
        await cache.append(test_key, data)

    elapsed = time.time() - start_time
    logger.info("Data append completed, elapsed time: %.2f seconds", elapsed)

    # 3. Verify data count
    size = await cache.get_queue_size(test_key)
    assert size == batch_size, f"Data count mismatch: expected {batch_size}, actual {size}"

    # 4. Test performance of getting large amount of data
    start_time = time.time()
    recent_data = await cache.get_by_timestamp_range(test_key)
    elapsed = time.time() - start_time

    assert len(recent_data) == batch_size, "Retrieved data count mismatch"
    logger.info("Retrieved %d items, elapsed time: %.2f seconds", len(recent_data), elapsed)

    # 5. Clean up test data
    success = await cache.clear_queue(test_key)
    assert success, "Failed to clean up test data"

    logger.info("✅ Stress test passed")


async def test_auto_cleanup():
    """Test automatic cleanup mechanism"""
    logger.info("Starting automatic cleanup test...")

    # Get cache manager factory from DI container
    factory = get_bean("redis_windows_cache_factory")
    cache = await factory.create_cache_manager(expire_minutes=1)  # 1 minute expiration

    test_key = "test_windows_cache_auto_cleanup"

    # 1. Clear test queue
    await cache.clear_queue(test_key)

    # 2. Add first message
    first_msg = {"id": 1, "content": "first message"}
    success = await cache.append(test_key, first_msg)
    assert success, "Failed to add first message"
    logger.info("Successfully added first message")

    # 3. Wait 40 seconds
    logger.info("Waiting 40 seconds...")
    await asyncio.sleep(40)

    # 4. Add second message
    second_msg = {"id": 2, "content": "second message"}
    success = await cache.append(test_key, second_msg)
    assert success, "Failed to add second message"
    logger.info("Successfully added second message")

    # 5. Wait 25 seconds before cleanup (first message now 55 seconds old, second message 15 seconds old)
    logger.info("Waiting 25 seconds before cleanup...")
    await asyncio.sleep(25)

    # 6. Manually trigger cleanup and verify result
    cleaned_count = await cache.cleanup_expired(test_key)
    logger.info("Cleanup completed, cleaned count: %d", cleaned_count)

    # 7. Get current data
    current_data = await cache.get_by_timestamp_range(test_key)
    logger.info("Remaining data count after cleanup: %d", len(current_data))

    # Note: Due to changes in cleanup logic, these assertions may need adjustment
    # We mainly verify that cleanup function works properly
    if len(current_data) > 0:
        logger.info("Remaining data: %s", [item["data"] for item in current_data])

    # Verify cleanup function executed at least
    assert cleaned_count >= 0, "Cleanup function execution error"

    logger.info("✅ Automatic cleanup test passed")


async def test_compatibility_layer():
    """Test backward compatibility layer"""
    logger.info("Starting backward compatibility layer test...")

    # Get default manager instance
    default_manager = get_bean("redis_windows_cache_manager")
    test_key = "test_windows_cache_compat"

    # 1. Clear test queue
    await default_manager.clear_queue(test_key)

    # 2. Test basic operations
    test_data = {"test": "compatibility"}
    success = await default_manager.append(test_key, test_data)
    assert success, "Backward compatibility layer append data failed"

    # Use new get by timestamp range method
    recent_data = await default_manager.get_by_timestamp_range(test_key)
    assert len(recent_data) == 1, "Backward compatibility layer get data failed"

    stats = await default_manager.get_queue_stats(test_key)
    assert stats["total_count"] == 1, "Backward compatibility layer statistics error"

    # 3. Test new timestamp range query method
    # Add some test data
    for i in range(2):
        data = {"index": i, "content": f"compat_data_{i}"}
        success = await default_manager.append(test_key, data)
        assert success, f"Backward compatibility layer add test data {i} failed"

    # Test get by timestamp range
    range_data = await default_manager.get_by_timestamp_range(test_key)
    assert (
        len(range_data) == 3
    ), f"Backward compatibility layer timestamp range query failed, expected 3, actual {len(range_data)}"  # 1 original data + 2 new data

    # Test limit count
    limited_data = await default_manager.get_by_timestamp_range(test_key, limit=2)
    assert len(limited_data) == 2, "Backward compatibility layer limit count function failed"

    logger.info("Backward compatibility layer new method test passed")

    # 4. Clean up test data
    success = await default_manager.clear_queue(test_key)
    assert success, "Backward compatibility layer clean up data failed"

    logger.info("✅ Backward compatibility layer test passed")


async def main():
    """Main test function"""
    logger.info("=" * 50)
    logger.info("Redis Time Window Cache Manager Test Started")
    logger.info("=" * 50)

    try:
        # Run all tests
        await test_json_pickle_mixed_data()
        await test_basic_operations()
        await test_timestamp_range_query()
        await test_pickle_performance()
        await test_stress_operations()
        await test_auto_cleanup()
        await test_compatibility_layer()

        logger.info("=" * 50)
        logger.info("✅ All tests passed")
        logger.info("=" * 50)

    except AssertionError as e:
        logger.error("❌ Test failed: %s", str(e))
        raise
    except Exception as e:
        logger.error("❌ Test error: %s", str(e))
        raise


if __name__ == "__main__":
    asyncio.run(main())